import copy
import math

import pandas as pd
import torch.multiprocessing
import time

from sklearn.model_selection import cross_val_score

from cfs_trans import Multi_p
from controller_alpha import Controller
import torch.optim as optim


from worker_alpha import Worker
import numpy as np
import torch
import logging
from get_reward import GetReward
from constant import NO_ACTION

import mindspore as ms
import mindspore.ops as ops
import mindspore.nn as nn
from mindspore import Parameter, Tensor


class PPO_ori(object):
    def __init__(self, args, nlp_feature=None):
        self.args = args
        self.process_pool_num = 24
        self.arch_epochs = 400
        self.arch_lr = 0.001
        self.episodes = 24
        self.entropy_weight = 1e-3

        self.task_type = args.task_type
        self.eval_method = args.eval_method

        self.ppo_epochs = 15
        # device = torch.device('cuda', args.cuda) if torch.cuda.is_available() and args.cuda != -1  else torch.device('cpu')
        device = ms.set_context(device_target='GPU')
        ms.set_context(pynative_synchronize=True)
        
        # self.controller = Controller(self.args).to(device)
        self.controller = Controller(self.args)
        self.get_reward_ins = GetReward(self.args, nlp_feature = nlp_feature)

        # self.adam = optim.Adam(params=self.controller.parameters(),
        #                        lr=self.arch_lr)
        self.adam = nn.Adam(params=self.controller.get_parameters(),learning_rate=self.arch_lr)
        self.baseline = None
        self.baseline_weight = 0.9
        self.clip_epsilon = 0.2
        self.search_data = None
        self.best_trans = None

    def get_base_score(self):
        # x = self.search_data.drop([self.args.target_col], axis = 1)
        # x = self.search_data[self.args.continuous_col + self.args.discrete_col]
        # y = self.search_data[self.args.target_col]
        # if self.task_type == 'classifier':
        #     model = ModelBase.rf_classify()
        #     score_list = cross_val_score(model, x, y, scoring="f1_micro", cv=5)
        # else:
        #     model = ModelBase.rf_regeression()
        #     subrae = make_scorer(sub_rae, greater_is_better=True)
        #     score_list = cross_val_score(model, x, y, scoring=subrae, cv=5)

        score_list = self.get_reward_ins.k_fold_score(self.search_data, NO_ACTION, is_base=True)
        return score_list

    def feature_search(self):
        # Initial score
        self.base_score = self.get_base_score()
        best_score = -10000000000000
        if self.eval_method == 'rmse' or self.eval_method == 'mse' or self.eval_method == 'mae':
            flag = -1
        else:
            flag = 1
        improve = []
        # =========================================================

        logging.info(f'base_score: {self.base_score}, '
                     f'base_score_mean: {np.mean(self.base_score)}, '
                     f'search_data_shape: {self.search_data.shape}')
        logging.info(f'controller structureï¼š{self.controller}')
        for arch_epoch in range(self.arch_epochs):
            last_time = time.time()
            
            def net_forward(best_score):
                # Sample actions and calculate the corresponding score
                # =========================================================
                sample_workers = []
                ids = []
                for try_num in range(self.episodes):
                    # Sample from network
                    tmp = []
                    sample_history = self.controller.sample()
                    # Instantiate worker
                    worker = Worker(self.args, sample_history)
                    tmp.append(worker.actions_trans)
                    tmp.append(try_num)
                    ids.append(tmp)
                    sample_workers.append(worker)
                    #print('test_point_worker', worker.actions_trans)
                multipro = Multi_p(self.args, self.search_data, self.get_reward_ins)
                ids = multipro.multi_c(self.process_pool_num, ids)
                for i in range(self.episodes):
                    tmp = ids[i]
                    worker = sample_workers[tmp[1]]
                    worker.score_list = tmp[0]
                    worker.rep_num = tmp[2]
                    worker.columns_name = tmp[3]
                    worker.fe_num = tmp[4]
                # =========================================================

                # Network inference action
                # =========================================================
                # infer_history = self.controller.inference()
                # infer_worker = Worker(self.args, infer_history)
                # infer_actions_trans = infer_worker.actions_trans
                # infer_actions_prob = infer_worker.actions_prob
                # infer_hp_actions = infer_worker.hp_action
                # =========================================================

                # Take the k-fold reward and average it into one. Each reward takes the current score - baseline
                # =========================================================
                for worker in sample_workers:
                    rep_score_list = []
                    for idx, single_score in enumerate(worker.score_list):
                        #
                        score = single_score - self.base_score[idx]
                        rep_score_list.append(score)
                    #worker.score = (np.mean(rep_score_list) - 0.01*worker.rep_num)
                    worker.score = (np.mean(rep_score_list))
                # =========================================================

                # Printout
                # =========================================================
                for worker in sample_workers:
                    mean_score = np.mean(worker.score_list)
                    if best_score < flag * mean_score:
                        best_score = flag * mean_score
                        self.best_trans = worker.actions_trans

                    logging.info(
                        f'rnn_cycle_num: {len(worker.actions_trans)}, '
                        f'sample_worker_score: {worker.score}, '
                        f'score_list: {worker.score_list}, '
                        f'score_list_mean: {np.mean(worker.score_list)}, '
                        f'fe_num:{worker.fe_num},'
                        f'columns_name:{worker.columns_name},'
                        f'worker_actions: {worker.actions_trans}'
                        #f'hp_action: {worker.hp_action}'
                    )
                # =========================================================

                # update baseline
                # =========================================================
                sample_score = [worker.score for worker in sample_workers]
                if self.baseline is None:
                    self.baseline = np.mean(sample_score)
                else:
                    for worker in sample_workers:
                        self.baseline = self.baseline * self.baseline_weight + \
                                        worker.score * (1-self.baseline_weight)
                improve.append(max(0, flag * self.baseline) / np.mean(self.base_score))
                #print(max(0, self.baseline) / self.base_score.mean())
                # =========================================================

                # Output infer related information
                # =========================================================
                logging.info(
                    f'\n ************************************************* \n'
                    f'data: {self.args.data} \n'
                    f'arch_epoch: {arch_epoch} \n'
                    f'baseline: {self.baseline} \n'
                    # f'infer_actions_len: {len(infer_actions_trans)} \n'
                    # f'infer_actions_trans: {infer_actions_trans} \n'
                    # f'infer_hp_actions: {infer_hp_actions} \n'
                    # f'infer_actions_prob: {infer_actions_prob} \n'
                    f' **************************************************')
                # =========================================================

                # Save the workers locally to facilitate the subsequent analysis and search process
                # =========================================================
                # sample_workers_info = []
                # for worker in sample_workers:
                #     sample_workers_info.append({
                #         'score': worker.score,
                #         'score_list': worker.score_list,
                #         'actions_trans': worker.actions_trans,
                #         'hp_actions': worker.hp_action
                #     })
                # total_workers.append({
                #     'sample_workers_info': sample_workers_info,
                #     'infer_actions_trans': infer_actions_trans,
                #     'infer_hp_actions': infer_hp_actions,
                #     'baseline': self.baseline,
                #     'base_score': self.base_score,
                #     'config_key': self.args.config_key,
                #     'with_combine_search': self.controller.with_combine_search,
                #     'with_hyper_param_search': self.controller.with_hyper_param_search
                # })
                # if self.args.exp_log_dir:
                #     save_path = \
                #         self.args.exp_log_dir / Path(str(self.args.config_key)+'.pkl')
                # else:
                #     save_path = self.args.exp_log_dir / Path('total_workers.pkl')
                # with open(save_path, 'wb') as f:
                #     pickle.dump(total_workers, f)
                # =========================================================

                # reward standardization
                # =========================================================
                score_lists = [worker.score for worker in sample_workers]
                score_reward = [score_list - self.baseline for score_list in score_lists]

                # Standardization of division standard deviation
                # ***************************************************
                # score_reward_std = np.std(score_reward)
                # score_reward_mean = np.mean(score_reward)
                # score_reward_normal = \
                #     [(reward - score_reward_mean) / (score_reward_std + 1e-6)
                #      for reward in score_reward]
                # ***************************************************

                # Maximum and minimum normalization without changing sign
                # ***************************************************
                score_reward_abs = [abs(reward) for reward in score_reward]
                min_reward = min(score_reward_abs)
                max_reward = max(score_reward_abs)
                score_reward_minmax = []
                for reward in score_reward:
                    if reward < 0:
                        min_max_reward = -(abs(reward) - min_reward) / \
                                         (max_reward - min_reward + 1e-6)
                    elif reward >= 0:
                        min_max_reward = (abs(reward) - min_reward) / \
                                         (max_reward - min_reward + 1e-6)
                    else:
                        min_max_reward = None
                        print('error')

                    score_reward_minmax.append(min_max_reward)
                score_reward_minmax = [i/2 for i in score_reward_minmax]
                # ***************************************************

                # Normalization mode selection
                normal_reward = score_reward_minmax
                # normal_reward = score_reward
                #normal_reward = score_reward_normal
                #print('normal_reward', normal_reward)

                # PPO network update
                # =========================================================
                # Cumulative gradient update method
                
                loss = 0
                for episode, worker in enumerate(sample_workers):
                    prod_history = self.controller.get_prod(worker.history)
                    loss += self.call_worker_loss(
                        prod_history, worker.history, normal_reward[episode])
                loss /= len(sample_workers)
                logging.info(f'training_rid: {ppo_epoch}, loss: {type(loss)}')
                return loss, best_score
            
            
            net_backward = ms.value_and_grad(net_forward, None, self.adam.parameters)
            for ppo_epoch in range(self.ppo_epochs):
                # #
                # loss.backward()
                #
                # self.adam.step()
                #
                # self.adam.zero_grad()
                (total_loss, best_score), grads = net_backward(best_score)
                self.adam(grads)
                
            now_time = time.time()
            logging.info(f'arch_epoch_time:{now_time-last_time}\n'f' **************************************************')
            # =========================================================
        logging.info(f'baseline_improve:{improve}\n')
        logging.info(f'best_score:{flag * best_score}\n' f'best_trans:{self.best_trans}\n')

    def call_worker_loss(self, prod_history, sample_history, reward):
        """
        Calculate loss function
        """
        policy_loss_list = []
        prod_actions_p = []
        sample_actions_p = []
        actions_entropy = []

        for ph in prod_history['feature_engineering']:
            prod_actions_p.append(ph['prob_vec'])
            actions_entropy.append(ph['action_entropy'])
        try:
            prod_actions_p.append(prod_history['hyper_parameter']['prob_vec'])
            actions_entropy.append(
                prod_history['hyper_parameter']['action_entropy'])
        except Exception as e:
            pass

        for sh in sample_history['feature_engineering']:
            sample_actions_p.append(sh['prob_vec'])
        try:
            sample_actions_p.append(
                sample_history['hyper_parameter']['prob_vec'])
        except Exception as e:
            pass

        # actions_entropy = torch.cat(actions_entropy, dim=0)
        # all_entropy = torch.sum(actions_entropy)
        #ms
        actions_entropy = ops.concat(actions_entropy,0)
        op = ops.ReduceSum(keep_dims=False)
        all_entropy = op(actions_entropy)

        for index, action_p in enumerate(prod_actions_p):
            action_importance = action_p / sample_actions_p[index]
            # print(action_importance)
            clipped_action_importance = self.clip(action_importance)

            action_reward = action_importance * reward
            clipped_action_reward = clipped_action_importance * reward

            # action_reward, _ = torch.min(
            #     torch.cat([action_reward.unsqueeze(0),
            #                clipped_action_reward.unsqueeze(0)], dim=0), dim=0)
            # policy_loss_list.append(torch.sum(action_reward))

            #ms
            expand_dims = ops.ExpandDims()
            
            a = expand_dims(clipped_action_reward,0)
            reshape_input = expand_dims(action_reward,0).resize(a.shape)
            action_reward= (ops.concat([reshape_input,a],0)).min()
            
            op = ops.ReduceSum(keep_dims=False)
            policy_loss_list.append(op(action_reward))

        if self.eval_method == 'mse' or self.eval_method == 'rmspe' \
                or self.eval_method =='rmse' or self.eval_method == 'mae':
            all_policy_loss = sum(policy_loss_list)
            entropy_bonus = all_entropy * self.entropy_weight
        elif self.eval_method == 'f1_score' or \
                self.eval_method == 'r_squared' or self.eval_method == 'acc' \
                or self.eval_method == 'auc' or self.eval_method == 'ks' or \
                self.eval_method == 'sub_rae':
            all_policy_loss = -1 * sum(policy_loss_list)
            entropy_bonus = -1 * all_entropy * self.entropy_weight
        else:
            logging.info(f'The evaluation index does not exist, it will be increased by default')
            all_policy_loss = -1 * sum(policy_loss_list)
            entropy_bonus = -1 * all_entropy * self.entropy_weight

        # print('loss: ', all_policy_loss, entropy_bonus)

        return all_policy_loss + entropy_bonus

    def clip(self, actions_importance):
        # lower = (torch.ones_like(actions_importance) * (1 - self.clip_epsilon))
        # upper = (torch.ones_like(actions_importance) * (1 + self.clip_epsilon))
        #ms
        lower = (ops.ones_like(actions_importance) * (1 - self.clip_epsilon))
        upper = (ops.ones_like(actions_importance) * (1 + self.clip_epsilon))

        # actions_importance, _ = torch.min(torch.cat(
        #     [actions_importance.unsqueeze(0), upper.unsqueeze(0)],
        #     dim=0), dim=0)
        # actions_importance, _ = torch.max(torch.cat(
        #     [actions_importance.unsqueeze(0), lower.unsqueeze(0)],
        #     dim=0), dim=0)

        #ms
        expand_dims = ops.ExpandDims()
        actions_importance = (ops.concat(
            [expand_dims(actions_importance,0), expand_dims(upper,0)],0)).min()
        # print("HERE",expand_dims(actions_importance,0).shape)
        # print("THERE",expand_dims(lower,0).shape)
        # breakpoint()
        Lower = expand_dims(lower,0)
        reshape_input = expand_dims(actions_importance,0).resize(Lower.shape)
        actions_importance = (ops.concat(
            [reshape_input,Lower],0)).max()

        return actions_importance


if __name__ == '__main__':
    load_args = 'hzd_amend'
    from main import args, dataset_config
    search_dataset_info = dataset_config[load_args]
    for key, value in search_dataset_info.items():
        setattr(args, key, value)

    ppo = PPO_ori(args)
    ppo.feature_search()



